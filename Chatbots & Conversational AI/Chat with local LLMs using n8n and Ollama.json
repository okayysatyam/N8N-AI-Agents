{
  "name": "Chat with local LLMs using n8n and Ollama",
  "nodes": [
    {
      "parameters": {
        "options": {}
      },
      "id": "0e87e849-5165-4b3c-b98e-ec87a2d0d515",
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "position": [
        544,
        384
      ],
      "webhookId": "ebdeba3f-6b4f-49f3-ba0a-8253dd226161",
      "typeVersion": 1.1
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "fbb050e5-3ca1-4486-9340-3c6546f4ddc9",
      "name": "Ollama Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOllama",
      "position": [
        752,
        608
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "content": "## Chat with local LLMs using n8n and Ollama\nThis n8n workflow allows you to seamlessly interact with your self-hosted Large Language Models (LLMs) through a user-friendly chat interface. By connecting to Ollama, a powerful tool for managing local LLMs, you can send prompts and receive AI-generated responses directly within n8n.\n\n### How it works\n1. When chat message received: Captures the user's input from the chat interface.\n2. Chat LLM Chain: Sends the input to the Ollama server and receives the AI-generated response.\n3. Delivers the LLM's response back to the chat interface.\n\n### Set up steps\n* Make sure Ollama is installed and running on your machine before executing this workflow.\n* Edit the Ollama address if different from the default.\n",
        "height": 473,
        "width": 485
      },
      "id": "2927bc4b-09bc-4a8a-971e-399f03ebf81c",
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        0,
        0
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "content": "## Ollama setup\n* Connect to your local Ollama, usually on http://localhost:11434\n* If running in Docker, make sure that the n8n container has access to the host's network in order to connect to Ollama. You can do this by passing `--net=host` option when starting the n8n Docker container",
        "height": 258,
        "width": 368,
        "color": 6
      },
      "id": "472dd9ab-51c9-4c2a-919a-8ae608fb4d83",
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        880,
        592
      ],
      "typeVersion": 1
    },
    {
      "parameters": {},
      "id": "630e9ffd-f4b8-4c86-97a0-64a9efe15387",
      "name": "Chat LLM Chain",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        768,
        384
      ],
      "typeVersion": 1.4
    }
  ],
  "pinData": {},
  "connections": {
    "Ollama Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "Chat LLM Chain",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "When chat message received": {
      "main": [
        [
          {
            "node": "Chat LLM Chain",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "b4fa42c6-199a-4849-be42-5e5662df3e9d",
  "meta": {
    "instanceId": "35ab32f3efc32e7770075c0bf6d587f0690d4c66ea03e863a40c1a522cbbe948"
  },
  "id": "cya6kKEOFQMEHhHV",
  "tags": []
}